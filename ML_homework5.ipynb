{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68c532b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attention Weights:\n",
      " [[0.26318647 0.25382501 0.23709717 0.24589135]\n",
      " [0.250044   0.27144639 0.22143687 0.25707274]\n",
      " [0.22109294 0.25979311 0.25117361 0.26794034]\n",
      " [0.20913057 0.26392171 0.23908665 0.28786108]]\n",
      "Context Vector:\n",
      " [[0.28188361 0.61726093 0.25250816 0.45623056 0.67824005 0.5678025\n",
      "  0.57651392 0.30752946]\n",
      " [0.27593331 0.60669449 0.25490398 0.46679666 0.68146233 0.57265\n",
      "  0.58056001 0.30896919]\n",
      " [0.26857128 0.62096175 0.24827066 0.45544712 0.70528679 0.58933699\n",
      "  0.59862346 0.29402693]\n",
      " [0.26334988 0.62219977 0.24621293 0.45936542 0.70997394 0.59105818\n",
      "  0.59678245 0.29253171]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    \"\"\"Compute softmax row-wise.\"\"\"\n",
    "    exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
    "\n",
    "def scaled_dot_product_attention(Q, K, V):\n",
    "    \"\"\"\n",
    "    Computes scaled dot-product attention.\n",
    "    \n",
    "    Args:\n",
    "        Q, K, V : numpy arrays of shape (seq_len, d_k)\n",
    "\n",
    "    Returns:\n",
    "        attention_weights: (seq_len, seq_len)\n",
    "        context_vector: (seq_len, d_k)\n",
    "    \"\"\"\n",
    "    d_k = Q.shape[-1]\n",
    "\n",
    "    # Step 1: Compute raw attention scores (QKᵀ)\n",
    "    scores = np.dot(Q, K.T)\n",
    "\n",
    "    # Step 2: Scale by sqrt(d_k)\n",
    "    scaled_scores = scores / np.sqrt(d_k)\n",
    "\n",
    "    # Step 3: Softmax over keys dimension\n",
    "    attention_weights = softmax(scaled_scores)\n",
    "\n",
    "    # Step 4: Compute context = Attention · V\n",
    "    context = np.dot(attention_weights, V)\n",
    "\n",
    "    return attention_weights, context\n",
    "\n",
    "\n",
    "# ----------- Example Test -------------\n",
    "if __name__ == \"__main__\":\n",
    "    Q = np.random.rand(4, 8)\n",
    "    K = np.random.rand(4, 8)\n",
    "    V = np.random.rand(4, 8)\n",
    "\n",
    "    attn_w, ctx = scaled_dot_product_attention(Q, K, V)\n",
    "    print(\"Attention Weights:\\n\", attn_w)\n",
    "    print(\"Context Vector:\\n\", ctx)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4059997f",
   "metadata": {},
   "source": [
    "## 1. Scaled Dot-Product Attention Output\n",
    "\n",
    "### Attention Weights:\n",
    "\n",
    "Each row represents how much a query token attends to all other tokens.  \n",
    "Softmax ensures each row sums to 1.  \n",
    "Higher values indicate stronger attention toward specific tokens.\n",
    "\n",
    "### Context Vector:\n",
    "\n",
    "Computed as the weighted sum of the value matrix.  \n",
    "Represents the updated embedding for each token after attention is applied.  \n",
    "Encodes information gathered from all other tokens in the sequence.\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "Attention weights determine which tokens influence each other.  \n",
    "Context vectors contain the resulting combined information, forming the final token representations after attention.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84af88a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: torch in c:\\users\\harip\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (2.9.1)\n",
      "Collecting torchvision\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchvision-0.24.1%2Bcpu-cp311-cp311-win_amd64.whl.metadata (6.1 kB)\n",
      "Collecting torchaudio\n",
      "  Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.9.1%2Bcpu-cp311-cp311-win_amd64.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\harip\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in c:\\users\\harip\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\harip\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\harip\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (3.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\harip\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in c:\\users\\harip\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torch) (2025.12.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\harip\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torchvision) (2.3.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\harip\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from torchvision) (11.3.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\harip\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\harip\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.11_qbz5n2kfra8p0\\localcache\\local-packages\\python311\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Downloading https://download.pytorch.org/whl/cpu/torchvision-0.24.1%2Bcpu-cp311-cp311-win_amd64.whl (4.0 MB)\n",
      "   ---------------------------------------- 0.0/4.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 4.0/4.0 MB 22.0 MB/s eta 0:00:00\n",
      "Downloading https://download.pytorch.org/whl/cpu/torchaudio-2.9.1%2Bcpu-cp311-cp311-win_amd64.whl (662 kB)\n",
      "   ---------------------------------------- 0.0/662.4 kB ? eta -:--:--\n",
      "   --------------------------------------- 662.4/662.4 kB 12.9 MB/s eta 0:00:00\n",
      "Installing collected packages: torchvision, torchaudio\n",
      "\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   ---------------------------------------- 0/2 [torchvision]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   -------------------- ------------------- 1/2 [torchaudio]\n",
      "   ---------------------------------------- 2/2 [torchaudio]\n",
      "\n",
      "Successfully installed torchaudio-2.9.1+cpu torchvision-0.24.1+cpu\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3034d716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape : torch.Size([32, 10, 64])\n",
      "Output shape: torch.Size([32, 10, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class SimpleEncoderBlock(nn.Module):\n",
    "    def __init__(self, embed_dim=64, num_heads=4, ff_dim=256):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Multi-head self-attention\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
    "        \n",
    "        # Feed-forward network\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # ---- Multi-head attention + Add & Norm ----\n",
    "        attn_output, _ = self.self_attn(x, x, x)\n",
    "        x = self.norm1(x + attn_output)\n",
    "\n",
    "        # ---- Feed-forward + Add & Norm ----\n",
    "        ff_output = self.ff(x)\n",
    "        x = self.norm2(x + ff_output)\n",
    "        return x\n",
    "\n",
    "\n",
    "# ---------- Shape Verification ----------\n",
    "if __name__ == \"__main__\":\n",
    "    batch_size = 32\n",
    "    seq_len = 10\n",
    "    embed_dim = 64\n",
    "\n",
    "    encoder = SimpleEncoderBlock(embed_dim=embed_dim)\n",
    "    sample_input = torch.randn(batch_size, seq_len, embed_dim)\n",
    "\n",
    "    output = encoder(sample_input)\n",
    "\n",
    "    print(\"Input shape :\", sample_input.shape)   # (32, 10, 64)\n",
    "    print(\"Output shape:\", output.shape)         # (32, 10, 64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d259bc7",
   "metadata": {},
   "source": [
    "## 2. Transformer Encoder Block Output\n",
    "\n",
    "### Input Shape:\n",
    "\n",
    "`(32, 10, 64)` → 32 samples, 10 tokens each, embedding size 64.\n",
    "\n",
    "### Output Shape:\n",
    "\n",
    "`(32, 10, 64)` → Same shape because:\n",
    "\n",
    "- Multi-head attention preserves embedding size  \n",
    "- Feed-forward layer expands → compresses back to 64  \n",
    "- Residual connections require matching dimensions  \n",
    "- LayerNorm does not change shape  \n",
    "\n",
    "### Interpretation\n",
    "\n",
    "The model changes the representation, not the shape, confirming the encoder block works correctly.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
